{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecI9SkWAlRxU"
      },
      "source": [
        "\n",
        "<br>\n",
        "<font>\n",
        "<!-- <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\"> -->\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
        "<br>\n",
        "<font color=0F5298 size=7>\n",
        "Machine Learning <br>\n",
        "<font color=2565AE size=5>\n",
        "Electrical Engineering Department <br>\n",
        "Spring 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "Practical Assignment 4 <br>\n",
        "<font color=696880 size=4>\n",
        "<!-- <br> -->\n",
        "\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttgagTyClRxV"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZRv3TmslRxV"
      },
      "outputs": [],
      "source": [
        "student_number = '400104686'\n",
        "first_name = 'Parsa'\n",
        "last_name = 'Yousefpoor'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT0zWff7lRxW"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMYpOokplRxW"
      },
      "source": [
        "In this assignment, we will be performing clustering on Spotify songs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJDm3x8plRxW"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3SkmPs5lRxW"
      },
      "source": [
        "In the next cell, import the libraries you'll need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sDPDRVuflRxW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"drive/My Drive/IML_4_2\")"
      ],
      "metadata": {
        "id": "fGXoE4SOqr8a",
        "outputId": "946d9cda-a4bc-4135-8e55-e27afb43c119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GtaWctZlRxW"
      },
      "source": [
        "In the `spotify.csv` file, load the data. Exclude unrelated features and retain only the track name and the features you believe are relevant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uJJ7tQOglRxW"
      },
      "outputs": [],
      "source": [
        "usecols = ['track_id','track_name', 'track_artist', 'track_album_id','track_album_name',\n",
        "                'track_album_release_date', 'playlist_name', 'playlist_id']\n",
        "df = pd.read_csv('spotify.csv', usecols = usecols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKcjMTgXlRxX"
      },
      "source": [
        "In this cell, you should implement a standard scalar function from scratch and applying it to your data. Explian importance behind using a standard scalar and the potential complications that could arise in clustering if it's not employed. (you can't use `sklearn.preprocessing.StandardScaler` but you are free to use `sklearn.preprocessing.LabelEncoder`)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "A Standard Scaler is crucial in data preprocessing, especially for machine learning algorithms that are sensitive to the scale of the data, such as PCA, LDA, and k-means clustering. Standard scaling ensures that each feature has a mean of 0 and a standard deviation of 1. This uniformity prevents features with larger scales from dominating the distance calculations and model training.\n",
        "\n",
        "**Complications Without Standard Scaling:**\n",
        "\n",
        "1. Bias in Distance-Based Algorithms: Algorithms like k-means clustering rely on distance metrics. Without scaling, features with larger ranges can disproportionately influence the clustering results.\n",
        "2. Convergence Issues: Gradient-based algorithms might converge more slowly or to suboptimal solutions if the feature scales vary significantly.\n",
        "3. Model Interpretability: Inconsistent feature scales can complicate the interpretation of model coefficients and predictions."
      ],
      "metadata": {
        "id": "A0-gj-BVrRej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wKNIw1ZclRxX",
        "outputId": "e14ffa2d-44a8-4742-c70b-cc01218dca6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 track_id                                         track_name  \\\n",
            "0  6f807x0ima9a1j3VPbc7VN  I Don't Care (with Justin Bieber) - Loud Luxur...   \n",
            "1  0r7CVbZTWZgbTCYdfa2P31                    Memories - Dillon Francis Remix   \n",
            "2  1z1Hg7Vb0AhHDiEmnDE79l                    All the Time - Don Diablo Remix   \n",
            "3  75FpbthrwQmzHlBJLuGdC7                  Call You Mine - Keanu Silva Remix   \n",
            "4  1e8PAfcKUYoKkxPhrHqw4x            Someone You Loved - Future Humans Remix   \n",
            "\n",
            "       track_artist          track_album_id  \\\n",
            "0        Ed Sheeran  2oCs0DGTsRO98Gh5ZSl2Cx   \n",
            "1          Maroon 5  63rPSO264uRjW1X5E6cWv6   \n",
            "2      Zara Larsson  1HoSmj2eLcsrR0vE9gThr4   \n",
            "3  The Chainsmokers  1nqYsOef1yKKuGOVchbsk6   \n",
            "4     Lewis Capaldi  7m7vv9wlQ4i0LFuJiE2zsQ   \n",
            "\n",
            "                                    track_album_name track_album_release_date  \\\n",
            "0  I Don't Care (with Justin Bieber) [Loud Luxury...               2019-06-14   \n",
            "1                    Memories (Dillon Francis Remix)               2019-12-13   \n",
            "2                    All the Time (Don Diablo Remix)               2019-07-05   \n",
            "3                        Call You Mine - The Remixes               2019-07-19   \n",
            "4            Someone You Loved (Future Humans Remix)               2019-03-05   \n",
            "\n",
            "  playlist_name             playlist_id  \n",
            "0     Pop Remix  37i9dQZF1DXcZDD7cfEKhW  \n",
            "1     Pop Remix  37i9dQZF1DXcZDD7cfEKhW  \n",
            "2     Pop Remix  37i9dQZF1DXcZDD7cfEKhW  \n",
            "3     Pop Remix  37i9dQZF1DXcZDD7cfEKhW  \n",
            "4     Pop Remix  37i9dQZF1DXcZDD7cfEKhW  \n",
            "Means of scaled features:\n",
            " Series([], dtype: float64)\n",
            "\n",
            "Standard deviations of scaled features:\n",
            " Series([], dtype: float64)\n"
          ]
        }
      ],
      "source": [
        "def standard_scaler(X):\n",
        "    mean = np.mean(X, axis=0)\n",
        "    std = np.std(X, axis=0)\n",
        "    scaled_X = (X - mean) / std\n",
        "    return scaled_X\n",
        "\n",
        "numeric_features = df.select_dtypes(include=[np.number]).columns\n",
        "df_scaled = df.copy()\n",
        "df_scaled[numeric_features] = standard_scaler(df[numeric_features])\n",
        "print(df_scaled.head())\n",
        "print(\"Means of scaled features:\\n\", df_scaled[numeric_features].mean())\n",
        "print(\"\\nStandard deviations of scaled features:\\n\", df_scaled[numeric_features].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JCpo5DrlRxX"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIC_9Jv2lRxX"
      },
      "source": [
        "One method for dimensionality reduction is Principal Component Analysis (PCA). Use its implementation from the `sklearn` library to reduce the dimensions of your data. Then, by using an appropriate cut-off for the `_explained_variance_ratio_` in the PCA algorithm, determine the number of principal components to retain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDtEUHESlRxX"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyuIGandlRxX"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFRMKRizlRxY"
      },
      "source": [
        "Implement K-means for clustering from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC-18_QdlRxY"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1stO0mz7lRxY"
      },
      "source": [
        "Using the function you've created to execute the K-means algorithm eight times on your data, with the number of clusters ranging from 2 to 9. For each run, display the genre of each cluster using the first two principal components in a plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JGs7GIrlRxY"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYa-J6fmlRxY"
      },
      "source": [
        "The Silhouette score and the Within-Cluster Sum of Squares (WSS) score are two metrics used to assess the quality of your clustering. You can find more information about these two methods [here](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb). Plot the Silhouette score and the WSS score for varying numbers of clusters, and use these plots to determine the optimal number of clusters (k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qUK1uvLlRxY"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-pDfIH9lRxY"
      },
      "source": [
        "# Checking Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTzVoU4SlRxY"
      },
      "source": [
        "To see how good was our clustering we will use a sample check and t-SNE method.\n",
        "\n",
        "first randomly select two song from every cluster and see how close these two songs are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq3BMNtFlRxZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHa4sip4lRxZ"
      },
      "source": [
        "Using t-SNE reduce dimension of data pointe to 2D and plot it to check how good datapoints are clustered (implementing this part is optional and have extra points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOEm7BoolRxZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}